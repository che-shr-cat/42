{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Intro\n",
        "\n",
        "This notebook uses the new [Gemini 2.0 Flash](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2#2.0-flash) to analyze arxiv research papers. We use the DeepSeek-R1 paper (https://arxiv.org/abs/2501.12948) as an example.\n",
        "\n",
        "You can compare results to the manually-curated blog post about DeepSeek-R1 here: https://gonzoml.substack.com/p/deepseek-r1-open-model-with-reasoning.\n",
        "\n"
      ],
      "metadata": {
        "id": "tnCwJI-XIRC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preps\n"
      ],
      "metadata": {
        "id": "_uPEjSYYI6bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ],
      "metadata": {
        "id": "xYA5lTrrV7fq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set Google Cloud project information and create client.\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ],
      "metadata": {
        "id": "JjBKMAXmV5Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "-8UIlZkmWAVv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_CLOUD_PROJECT = 'api-project-616886193868'    # !!! CHANGE IT FOR YOUR OWN PROJECT\n",
        "GOOGLE_CLOUD_LOCATION = 'us-central1'                # !!! ... AND REGION"
      ],
      "metadata": {
        "id": "X7aOWXIvtr-K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai.types import HttpOptions, Part\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "0QoNnT0JuL5-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(vertexai=True, project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_LOCATION)"
      ],
      "metadata": {
        "id": "SswRuWY5uNp5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Gemini 2.0 Flash model, namely version 001. More about Gemini 2.0 models [here](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2)."
      ],
      "metadata": {
        "id": "5mlWCz7XW2rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gemini-2.0-flash-001\""
      ],
      "metadata": {
        "id": "khmpCnE33tB5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the paper"
      ],
      "metadata": {
        "id": "_zZWDNZ83m8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini 2.0 Flash supports [Document understanding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/document-understanding) and can work with PDF files.\n",
        "\n",
        "The PDF files need to be uploaded to a bucket in Google Cloud first. I did it manually outside of the notebook."
      ],
      "metadata": {
        "id": "6cNS990lXSKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSeek-R1 paper, original source\n",
        "paper_url_source = 'https://arxiv.org/pdf/2501.12948'\n",
        "\n",
        "# the link we work with\n",
        "paper_url = 'https://storage.googleapis.com/arxiv-papers-copy/2501.12948v1.pdf'"
      ],
      "metadata": {
        "id": "0MGtp8hu3w3w"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = Part.from_uri(\n",
        "    file_uri=paper_url,\n",
        "    mime_type=\"application/pdf\",\n",
        ")"
      ],
      "metadata": {
        "id": "9MxrQ28hvGlz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later, when the [Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) feature will work with Gemini 2.0 it would make sense to start using it. Right now, Context caching only supports Gemini 1.5."
      ],
      "metadata": {
        "id": "wjpvjzNVXyV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High-level summary"
      ],
      "metadata": {
        "id": "FdM-7Lu_v7-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the Gemini 2.0 Flash capabilities and ensure everything works with PDF files.\n",
        "\n",
        "Here we ask the model for a short summary of the paper:"
      ],
      "metadata": {
        "id": "OSKnLdTxYKGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are a highly skilled document summarization specialist. Your task is to provide a concise executive summary of no more than 300 words. Please summarize the given document for a general audience.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eAZ0HWahve2P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=[pdf_file, prompt],\n",
        ")"
      ],
      "metadata": {
        "id": "fMEb7E3Jvi0i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how quickly the model processed the whole 22-pages file!"
      ],
      "metadata": {
        "id": "yJDJQrwxYyI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "IH5PatGW8g1P",
        "outputId": "97752325-595d-4f28-f17c-00012b6e52cc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "DeepSeek-AI introduces DeepSeek-R1, a novel approach to enhance reasoning capabilities in Large Language Models (LLMs) through reinforcement learning (RL). The research explores the potential of LLMs to develop reasoning skills without supervised fine-tuning (SFT), emphasizing self-evolution via RL.\n\nThe study starts with DeepSeek-R1-Zero, directly applying RL to a base model. It demonstrates remarkable reasoning behaviors emerging naturally through the RL process. This pure RL approach validates that LLMs can be incentivized to reason effectively without prior SFT, marking a significant milestone.\n\nTo address limitations of DeepSeek-R1-Zero, like readability and language mixing, DeepSeek-R1 incorporates a multi-stage training pipeline, including a small amount of cold-start data before RL. This involves collecting high-quality data for initial fine-tuning, followed by reasoning-oriented RL. Furthermore, new SFT data is created using rejection sampling, enhancing capabilities in various domains. After which the model undergoes another RL phase to consider prompts from all scenarios. DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on reasoning tasks.\n\nThe researchers further investigate distilling the reasoning patterns from DeepSeek-R1 into smaller, more efficient models. Distillation proves more effective than directly applying RL to smaller base models, highlighting the importance of reasoning patterns discovered by larger models. DeepSeek-AI has made DeepSeek-R1-Zero, DeepSeek-R1, and six dense models, distilled from DeepSeek-R1, openly accessible to the research community.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summary is quite good!\n",
        "\n",
        "It is definitely more useful than the paper's own abstract:\n",
        "\n",
        "> \"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\"\n",
        "\n",
        "I especially like that the model highlighted an important point I also highlighted. It is that distillation proved to be more effective than direct RL.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9eUNaLhDZP5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More specific questions to the paper"
      ],
      "metadata": {
        "id": "NuTzzjrNv-au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask more questions to the paper.\n",
        "\n",
        "I've prepared a set of general questions worth considering when you analyze a new paper. Feel free to adjust the list as you need. These questions give a broad overview and assume some amount of reasoning.\n",
        "\n",
        "In the future, when the [Gemini 2.0 Flash Thinking](https://cloud.google.com/vertex-ai/generative-ai/docs/thinking#thoughts) model will support document understanding, it would be interesting to compare the results."
      ],
      "metadata": {
        "id": "Mg6w-qILYjCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are a highly skilled PhD-level AI/ML expert. Your task is to provide a concise answers of no more than 300 words. Please answer the following question for the professional audience: {question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DseW_WDq9afl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "  {\n",
        "    \"category\": \"motivation_and_context\",\n",
        "    \"question\": \"What is the central problem or challenge the paper aims to address?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"motivation_and_context\",\n",
        "    \"question\": \"Why is this problem important and what gap in the current literature does the paper fill?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"contributions_and_novelty\",\n",
        "    \"question\": \"What are the key contributions and innovations introduced in the paper?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"contributions_and_novelty\",\n",
        "    \"question\": \"How does the proposed approach differ from or improve upon previous methods?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"methodology_and_technical_approach\",\n",
        "    \"question\": \"What is the overall framework or algorithm used in this work?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"methodology_and_technical_approach\",\n",
        "    \"question\": \"Are the technical details (e.g., training methods, architecture, optimization strategies) clearly explained and justified?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"methodology_and_technical_approach\",\n",
        "    \"question\": \"How does the paper balance theoretical insights with practical implementation (e.g., use of reinforcement learning, supervised fine-tuning, or a hybrid approach)?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"experimental_setup_and_evaluation\",\n",
        "    \"question\": \"What datasets, benchmarks, or tasks are used to validate the proposed method?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"experimental_setup_and_evaluation\",\n",
        "    \"question\": \"Are the experiments designed rigorously to support the authors' claims?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"experimental_setup_and_evaluation\",\n",
        "    \"question\": \"How does the performance compare with state-of-the-art methods or established baselines?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"experimental_setup_and_evaluation\",\n",
        "    \"question\": \"Are there any ablation studies or error analyses that help understand the factors driving the performance?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"analysis_and_interpretability\",\n",
        "    \"question\": \"Does the paper provide insights into why the proposed method works (e.g., through qualitative examples, 'aha moments', or emergent behaviors)?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"analysis_and_interpretability\",\n",
        "    \"question\": \"How well can the approach be interpreted or explained to a non-expert audience?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"limitations_and_future_directions\",\n",
        "    \"question\": \"What are the current limitations or potential weaknesses of the approach?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"limitations_and_future_directions\",\n",
        "    \"question\": \"Are there suggestions for future research or open questions that remain?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"broader_impact_and_reproducibility\",\n",
        "    \"question\": \"What are the broader implications of the work for the research community, industry, or society?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"broader_impact_and_reproducibility\",\n",
        "    \"question\": \"Are ethical, safety, or deployment considerations discussed in the paper?\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"broader_impact_and_reproducibility\",\n",
        "    \"question\": \"Is the work reproducible? Are the code, data, and experimental settings sufficiently detailed or openly available?\"\n",
        "  }\n",
        "]\n"
      ],
      "metadata": {
        "id": "9ukjCGeWvkxc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(len(questions)), desc=\"Processing questions\"):\n",
        "    question_data = questions[i]\n",
        "    if \"answer\" not in question_data:\n",
        "        current_question = question_data[\"question\"]\n",
        "        response = client.models.generate_content(\n",
        "            model=model_id,\n",
        "            contents=[pdf_file, prompt.format(question=current_question)],\n",
        "        )\n",
        "        question_data[\"answer\"] = response.text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqQsJ7GX84aP",
        "outputId": "217613ee-ee47-4722-df1e-a3b5368108a7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing questions: 100%|██████████| 18/18 [02:02<00:00,  6.82s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, notice the speed! 18 questions were processed in just two minutes. Just 7 seconds for a question across the 22-page document!"
      ],
      "metadata": {
        "id": "G8Z1Oae7aqTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display digest\n",
        "\n",
        "Now we are ready to display all the model answers and compare with the [human (my own) baseline](https://gonzoml.substack.com/p/deepseek-r1-open-model-with-reasoning)."
      ],
      "metadata": {
        "id": "W8zRpj3JbLGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_questions(questions):\n",
        "\n",
        "    for question_data in questions:\n",
        "        question = question_data[\"question\"]\n",
        "        answer = question_data.get(\"answer\", \"No answer available.\")\n",
        "        display(Markdown('##'+question))\n",
        "        display(Markdown(answer))"
      ],
      "metadata": {
        "id": "eMT6w9lnAFRM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_questions(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ayrkBkwkF7DH",
        "outputId": "1a0f3a17-b0ed-4d8e-f651-fc97e94bf6d3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##What is the central problem or challenge the paper aims to address?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The DeepSeek-R1 paper addresses the challenge of **incentivizing robust reasoning capabilities in large language models (LLMs) without relying on extensive supervised fine-tuning (SFT)**. While SFT is a common technique, the authors explore whether powerful reasoning can emerge directly through reinforcement learning (RL).\n\nThe central problem is two-fold:\n\n1.  **Can an LLM develop complex reasoning abilities through RL alone, bypassing the need for labeled reasoning data?** This is investigated with DeepSeek-R1-Zero, a model trained solely with RL.\n2.  **How can an RL-trained LLM be further improved to achieve human-aligned reasoning and strong general capabilities?** This is addressed with DeepSeek-R1, which incorporates a small amount of \"cold-start\" data and a multi-stage RL pipeline.\n\nThe paper also delves into the problem of efficiently transferring reasoning capabilities to smaller models via distillation, allowing them to achieve better performance than RL-trained small models. Ultimately, the paper seeks to demonstrate a comprehensive methodology for developing highly capable reasoning LLMs through RL and distillation, reducing the reliance on resource-intensive supervised methods.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##Why is this problem important and what gap in the current literature does the paper fill?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This paper addresses a critical challenge in the field of large language models (LLMs): incentivizing reasoning capability without relying heavily on supervised fine-tuning (SFT). The problem is important because SFT, while effective, requires extensive human-annotated data, which is expensive and limits scalability and potentially introduces bias. Furthermore, relying primarily on SFT may stifle the model's ability to explore novel reasoning strategies independently.\n\nThe paper tackles this by introducing DeepSeek-R1, trained through reinforcement learning (RL), demonstrating remarkable reasoning capabilities without initial SFT. This underscores that reasoning can emerge organically through RL, unlocking significant potential for self-evolution in LLMs. Addressing the limitation of readability and language mixing of DeepSeek-R1-Zero, DeepSeek-R1 incorporates multi-stage training and cold-start data before RL.\n\nThe paper fills a significant gap in the current literature by:\n\n1.  **Validating RL-Only Reasoning:** Explicitly demonstrating that pure RL can successfully incentivize sophisticated reasoning behaviors in LLMs, challenging the prevalent reliance on SFT.\n2.  **A Novel Training Pipeline:** Introducing a multi-stage training pipeline, which allows the models to improve reasoning patters and align with human preferences.\n3.  **Distillation Insights:** Demonstrating that distilling reasoning patterns from large, RL-trained models can lead to smaller, highly capable models, which are beneficial for downstream tasks and accessibility to a broader research community. The release of the distilled models allows researchers to continue to improve them.\n\nIn essence, the paper pushes the boundaries of LLM training by showcasing the power of RL in fostering autonomous reasoning and introducing new possibilities for creating more efficient reasoning models.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##What are the key contributions and innovations introduced in the paper?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper \"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\" presents several key contributions to the field of language model reasoning.\n\n1.  **Pure RL-based Reasoning:** It demonstrates that strong reasoning capabilities can emerge in LLMs through reinforcement learning (RL) *without* supervised fine-tuning (SFT). This is significant because it validates that reasoning can be incentivized directly through RL, reducing reliance on expensive curated datasets. The initial model, DeepSeek-R1-Zero, showcases this emergent reasoning.\n2.  **Multi-Stage RL Pipeline:** The paper introduces a novel multi-stage training pipeline for DeepSeek-R1. This involves an initial cold-start SFT with high-quality reasoning examples, followed by reasoning-oriented RL, then rejection sampling for SFT data creation and subsequent RL aligning model with human preferences.\n3.  **Distillation of Reasoning Skills:** It demonstrates that reasoning patterns learned by larger models can be effectively distilled into smaller, more efficient models through SFT. The paper opensources distilled models based on Qwen and Llama, significantly surpassing existing open-source alternatives on reasoning benchmarks.\n4. **Novel Application of GRPO:** The paper demonstrates the efficient use of Group Relative Policy Optimization (GRPO) to accelerate the training phase of RL.\n\nIn essence, the paper highlights RL as a powerful tool for developing reasoning capabilities, offers a practical training methodology and emphasizes the value of distilling knowledge from larger to smaller models.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##How does the proposed approach differ from or improve upon previous methods?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The DeepSeek-R1 paper presents advancements in incentivizing reasoning capabilities in LLMs via reinforcement learning (RL), differing from previous methods in several key aspects.\n\nFirstly, DeepSeek-R1-Zero demonstrates that LLMs can develop remarkable reasoning skills through *pure RL*, without relying on supervised fine-tuning (SFT) as a preliminary step. This contrasts with many prior works that typically use SFT to bootstrap reasoning abilities before RL alignment. This highlights RL's potential to independently discover reasoning patterns.\n\nSecondly, DeepSeek-R1 incorporates a *multi-stage training pipeline* involving cold-start data, reasoning-oriented RL, rejection sampling, and a second SFT and RL stage focused on general capabilities (helpfulness/harmlessness). This holistic approach contrasts with simpler, single-stage RL fine-tuning strategies found in some prior works.\n\nThirdly, the paper emphasizes *distilling reasoning capabilities into smaller models* post-RL, demonstrating that the discovered reasoning patterns from larger models can be effectively transferred.  Direct distillation from DeepSeek-R1 outperformed RL on smaller models.\n\nFinally, while previous work explores process-based reward models (PRM) and Monte Carlo Tree Search (MCTS), the authors found them less effective compared to rule-based accuracy and language consistency rewards during RL and the improved performance by applying the distillation approach.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##What is the overall framework or algorithm used in this work?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper introduces DeepSeek-R1, a language model (LLM) designed to enhance reasoning capabilities through Reinforcement Learning (RL). The core framework revolves around a multi-stage training pipeline leveraging both supervised fine-tuning (SFT) and RL.\n\nInitially, a base model (DeepSeek-V3-Base) undergoes SFT using a small dataset of high-quality, long Chain-of-Thought (CoT) examples. This \"cold start\" aims to provide a foundation for reasoning. The model then enters a reasoning-oriented RL phase, optimizing for accuracy and language consistency in tasks like coding, mathematics, and logic. GRPO method is used for saving the cost of RL training.\n\nFollowing RL convergence, a rejection sampling technique is employed on the resulting checkpoint to generate new SFT data covering various domains, including writing, role-playing, and general-purpose tasks. This expanded dataset is used for another round of SFT on DeepSeek-V3-Base.\n\nFinally, a second RL stage is implemented, accounting for prompts from diverse scenarios to improve helpfulness and harmlessness while refining reasoning skills. This iterative process leads to the DeepSeek-R1 model.\n\nThe work also explores distillation, where the learned reasoning patterns of DeepSeek-R1 are transferred to smaller, open-source models (Qwen and Llama) through direct fine-tuning on the generated data.\n\nOverall, the framework relies on an iterative approach alternating between supervised fine-tuning and reinforcement learning to improve both reasoning capabilities and human alignment for the models.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##Are the technical details (e.g., training methods, architecture, optimization strategies) clearly explained and justified?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The DeepSeek-R1 paper provides a reasonable level of detail regarding its technical aspects, but with some areas warranting further clarity.\n\n**Strengths:** The paper clearly outlines the two-stage training approach, highlighting the use of GRPO for RL and incorporating cold-start data. The motivation for using rule-based rewards in DeepSeek-R1-Zero to avoid reward hacking is well-justified. The data curation process for cold-start and SFT, including the emphasis on readability and filtering, is also described.\n\n**Areas for Improvement:** While GRPO is mentioned, a more in-depth explanation of its implementation specifics (e.g., group size, baseline estimation) would benefit experts. Similarly, details on the architecture of DeepSeek-V3-Base, which serves as the foundation, are missing. The language consistency reward is introduced, but the method of calculation and its weighting within the overall reward function could be elaborated upon. Additionally, information on hyperparameter tuning strategies and compute resources used for training would be beneficial.\n\n**Overall:** The paper strikes a balance between providing sufficient technical details to understand the methodology and maintaining conciseness. While certain aspects could be further elaborated, the core techniques and their justifications are presented clearly enough for AI/ML experts to grasp the overall approach. The paper's focus on high-level innovations and empirical results likely drove the decision to omit excessively granular technical specifications.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##How does the paper balance theoretical insights with practical implementation (e.g., use of reinforcement learning, supervised fine-tuning, or a hybrid approach)?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The DeepSeek-R1 paper effectively balances theoretical reinforcement learning (RL) concepts with practical language model implementation through a multi-stage training pipeline.\n\nTheoretically, the paper explores the potential of RL for emergent reasoning without supervised fine-tuning (SFT), demonstrating that RL alone can incentivize complex reasoning behaviors in LLMs (DeepSeek-R1-Zero). This addresses the fundamental question of whether reasoning can arise purely from reward-driven learning, bypassing the need for curated datasets.\n\nPractically, the approach is implemented using the GRPO algorithm to improve model performance in reasoning. To mitigate issues from pure RL like poor readability and language mixing, the paper introduces DeepSeek-R1 which incorporates a cold-start phase of limited supervised data, as well as multi-stage training.\n\nThis hybrid approach blends theoretical RL with the practical benefits of SFT to achieve state-of-the-art results. The paper also explores distillation, transferring the reasoning capabilities of larger models into smaller, more efficient ones through supervised fine-tuning.\n\nOverall, the paper demonstrates a keen awareness of both the theoretical underpinnings of RL and the pragmatic considerations necessary for deploying high-performing language models, striking a balance between exploration and exploitation in the design of its training methodology.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##What datasets, benchmarks, or tasks are used to validate the proposed method?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The DeepSeek-R1 paper validates its approach to incentivizing reasoning capabilities in LLMs using a comprehensive suite of datasets and benchmarks, encompassing reasoning, coding, and knowledge-based tasks.\n\nFor *reasoning tasks*, the models were evaluated on:\n*   AIME 2024\n*   MATH-500\n*   CNMO 2024\n\nFor *coding-related tasks*, they used:\n*   Codeforces (percentile and rating)\n*   LiveCodeBench\n*   SWE-bench Verified\n*   Aider-Polyglot\n\n*Knowledge-based evaluation* included\n*   MMLU,\n*   MMLU-Redux\n*   MMLU-Pro\n*   GPQA Diamond\n*   SimpleQA\n*   FRAMES\n\nThe paper also includes benchmarks such as AlpacaEval 2.0 and Arena-Hard to evaluate open-ended generation and alignment with human preferences. The focus is on showing superior or competitive performance against established baselines like OpenAI-01-mini, OpenAI-01-1217, DeepSeek-V3, and other open-source models."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##Are the experiments designed rigorously to support the authors' claims?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The DeepSeek-R1 paper presents compelling results, and while the experimental design is generally sound, there are areas that could benefit from greater rigor.\n\nStrong points include the comprehensive benchmark suite covering various reasoning tasks (math, coding, knowledge). The comparison against strong baselines, including OpenAI's models, provides valuable context. The ablation studies on the impact of different training stages (RL, SFT) are informative.\n\nHowever, the paper could be strengthened by:\n\n1.  **More detailed hyperparameter tuning information.** The paper lacks specifics regarding hyperparameter selection for both SFT and RL stages. Reporting a wider range of values would allow for reproducibility.\n2.  **Analysis on the impact of data contamination.** The benchmarks used are common, which raises concerns regarding data contamination. The authors should address the possible overlap between training data and evaluation sets.\n3.  **Statistical significance testing.** While accuracy is reported, statistical significance testing is largely absent. Including p-values would help confirm if the reported differences are statistically significant.\n4.  **Fairness and Bias analysis.** Analysis of the models fairness and potential biases is missing.\n\nOverall, the experimental design adequately supports the authors' claims, but more details concerning hyperparameter tuning, data contamination, statistical significance, and fairness would be beneficial.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##How does the performance compare with state-of-the-art methods or established baselines?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The DeepSeek-R1 paper presents a significant advancement in reasoning capabilities for large language models (LLMs), achieving performance comparable to OpenAI's `o1-1217` on reasoning tasks. Here's a breakdown of its comparative performance against baselines:\n\n**DeepSeek-R1 vs. OpenAI `o1-1217`:**\n\n*   Comparable performance on tasks like MATH-500 (97.3% Pass@1) and AIME 2024 (79.8% Pass@1), slightly surpassing `o1-1217` on AIME.\n\n*   Excellent knowledge, but slightly below of `o1-1217` on MMLU, MMLU-Pro, and GPQA Diamond benchmarks\n\n**DeepSeek-R1 vs. DeepSeek-V3 (and other closed-source models):**\n\n*   Superior performance on knowledge benchmarks (MMLU, GPQA), FRAMES, and the factual benchmark SimpleQA. Outperforming the base model (DeepSeek-V3) by significant margins.\n\n*   Demonstrates enhanced performance on the instruction-following IFEval.\n\n**DeepSeek-R1-Zero vs. OpenAI `o1-0912`:**\n\n*   After RL DeepSeek-R1-Zero significantly increases the pass@1 score on AIME 2024 reaching comparable score with OpenAI-01-0912\n\n**Distilled DeepSeek-R1 models vs. Open-Source LLMs:**\n\n*   Distilled smaller dense models (Qwen and Llama series) significantly outperform previous open-source models, such as QwQ-32B-Preview, with the distilled 14B setting a new performance benchmark.\n\n**Overall:**\n\nDeepSeek-R1 demonstrates robust performance that rivals current state-of-the-art commercial models, while also providing open-source models competitive with or exceeding other comparable open source.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##Are there any ablation studies or error analyses that help understand the factors driving the performance?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the DeepSeek-R1 paper, while it doesn't explicitly label sections as \"ablation studies\" or \"error analyses\" in the traditional sense, there are several elements that contribute to understanding the factors driving performance:\n\n1.  **Comparison of DeepSeek-R1-Zero vs. DeepSeek-R1:** Comparing the performance of the zero-shot RL model (R1-Zero) against the model with cold-start data (R1) highlights the impact of even a small amount of high-quality supervised data on reasoning and convergence speed.\n\n2.  **Language Consistency Reward:** Introducing a language consistency reward during RL training demonstrates a trade-off: slight performance degradation in benchmarks but improved readability, suggesting a controllable balance between accuracy and human preference.\n\n3.  **Distillation Analysis:** The paper compares RL vs SFT-based distillation, and it provides performance comparisons of models trained with both strategies. This highlights that distilling larger pre-trained models to smaller ones, followed by Supervised fine-tuning, yields more improved results.\n\n4.  **Unsuccessful Attempts:** Discussing failures with PRM and MCTS provides insights into the challenges of fine-grained reward modeling and scalable test-time computation in reasoning. This implicitly points to the effectiveness of the chosen RL approach with rule-based rewards and cold-start data.\n\nWhile a formal error analysis (e.g., categorizing failure cases) is absent, these elements collectively provide a nuanced understanding of the individual contributions of different training stages and techniques to the final DeepSeek-R1 performance.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##Does the paper provide insights into why the proposed method works (e.g., through qualitative examples, 'aha moments', or emergent behaviors)?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, the paper provides several insights into why DeepSeek-R1 achieves improved reasoning capabilities.\n\n**\"Aha\" Moments and Emergent Behaviors:** The paper highlights the emergence of \"aha moments\" during DeepSeek-R1-Zero training. These moments represent instances where the model autonomously learned to allocate more thinking time by reevaluating its initial approach to a problem. The spontaneous emergence of reflection (revisiting and reevaluating previous steps) and exploration of alternative problem-solving approaches as test-time computation increased are also presented as key emergent behaviors.\n\n**Increased Thinking Time:** The analysis reveals that DeepSeek-R1-Zero naturally learns to allocate more thinking time to reasoning tasks. This intrinsic development allows the model to solve increasingly complex problems by generating hundreds to thousands of reasoning tokens, facilitating a more thorough exploration of thought processes.\n\n**Distillation Benefits:** The paper demonstrates that the reasoning patterns discovered by larger models like DeepSeek-R1 can be effectively distilled into smaller models. These distilled models outperform those trained with RL alone, suggesting that the specific reasoning patterns learned are crucial for improving capabilities.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##How well can the approach be interpreted or explained to a non-expert audience?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The DeepSeek-R1 paper presents advancements in language model reasoning capabilities through reinforcement learning (RL). While the technical details (GRPO, reward modeling, distillation) are complex, the core idea can be communicated effectively to a non-expert audience.\n\nThe key concept is training AI models to \"think better\" through a system of rewards, similar to training a pet with treats. Instead of direct instruction (supervised learning), the model learns to solve problems by trying different approaches and receiving \"rewards\" for correct answers and well-structured reasoning. DeepSeek-R1 builds upon this by incorporating a \"cold start\" – a basic understanding before the reward system is applied – analogous to teaching a child some basic rules before letting them explore.\n\nFurthermore, the paper explores \"distillation,\" which is like extracting the knowledge from a highly experienced teacher (large model) and transferring it to a smaller, more efficient student (smaller model). This makes the sophisticated reasoning abilities accessible in smaller, more practical AI models.\n\nThe results show improved performance on challenging reasoning tasks, demonstrating the effectiveness of the RL approach. While the inner workings are opaque, the high-level concepts of incentivizing reasoning and knowledge transfer are readily understandable, making the approach explainable in principle, even if specific implementation details remain inaccessible to non-experts. The potential for improved reasoning capabilities in AI systems is the central takeaway.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##What are the current limitations or potential weaknesses of the approach?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The DeepSeek-R1 paper presents a compelling approach to incentivizing reasoning capabilities in LLMs through reinforcement learning. However, the approach has limitations and weaknesses.\n\n**Generalization & Scaling of RL:** The RL training appears heavily focused on specific benchmarks. Over-optimization on these benchmarks might lead to reduced generalization to unseen reasoning tasks or real-world scenarios. The paper also notes the heavy compute requirements of large-scale RL.\n\n**Dependence on Base Model and Data Quality:** The success hinges on a strong base model (DeepSeek-V3) and the quality of the cold-start data used for fine-tuning. Biases or limitations within these components could propagate into the final model. Though attempts are made to filter, outputs may still be challenging to read and contain long paragraphs. \n\n**Lack of Rigorous Ablation Studies:** While the paper describes a multi-stage training pipeline, detailed ablation studies isolating the impact of each stage (cold-start data, reasoning-oriented RL, rejection sampling) are somewhat lacking. This makes it harder to understand the individual contribution of each element.\n\n**Potential for Reward Hacking:** Though the authors avoided neural reward models, reliance on rule-based rewards can still lead to reward hacking if not carefully designed and monitored.\n\n**Sensitivity to Prompting:** The paper mentions the model's sensitivity to prompting strategies. This requires expert knowledge to use effectively.\n\n**Language Mixing & Other Tasks:** Language mixing remains a problem, as well as challenges in tasks such as function calling, multi-turn, complex role-playing, and JSON output.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##Are there suggestions for future research or open questions that remain?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, the DeepSeek-R1 paper highlights several avenues for future research and open questions:\n\n1.  **Improving General Capabilities:** The paper identifies limitations in tasks like function calling, multi-turn conversations, complex role-playing, and generating JSON output compared to DeepSeek-V3. Future work could explore how to better leverage long Chain-of-Thought (CoT) reasoning to enhance performance in these areas.\n\n2.  **Addressing Language Mixing:** The model currently struggles with language mixing issues when handling queries in languages other than Chinese and English. Research is needed to improve multilingual reasoning capabilities and ensure consistent language use across different contexts.\n\n3.  **Prompt Engineering Sensitivity:** The model's performance is sensitive to prompting styles, with few-shot prompting degrading results. Investigating more robust prompting techniques and developing methods for zero-shot generalization are crucial.\n\n4.  **Software Engineering Tasks:** Scaling reinforcement learning to improve performance on software engineering benchmarks is a challenge due to long evaluation times. Future research should explore rejection sampling on software engineering data and asynchronous evaluation methods to improve efficiency.\n\n5.  **RL HF tradeoff**: While fine-tuning RL increases performance, in the Chinese SimpleQA benchmark, DeepSeek-R1 performs worse than DeepSeek-V3 due to refusal to answer queries after the safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. Thus, what is the optimal balance between RL and HF?\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##What are the broader implications of the work for the research community, industry, or society?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This DeepSeek-R1 research, centered on incentivizing reasoning in LLMs through reinforcement learning, has significant implications:\n\n**Research Community:** The findings demonstrate that strong reasoning capabilities can emerge organically in LLMs via RL without relying on initial supervised fine-tuning. This challenges the conventional wisdom and opens up a new avenue for exploration. The open-sourcing of the models and datasets enables further investigation into RL-driven reasoning, cold-start strategies, and distillation techniques.\n\n**Industry:** The success of distillation in transferring reasoning abilities to smaller, more efficient models is highly relevant to industry. Deploying smaller models with strong reasoning capabilities is cost-effective and energy-efficient. The methodology offers a pathway to create specialized, high-performing models for specific reasoning-intensive applications (coding, mathematics, scientific reasoning). Furthermore, the robust multi-stage training pipeline and the discovery of key training components (cold-start data, language consistency reward) offers a valuable roadmap for companies aiming to build LLMs.\n\n**Society:**  Improved reasoning capabilities in LLMs can lead to more reliable and accurate AI-driven tools. Education, problem-solving, and decision-making tasks can potentially be significantly improved by RL and distillation.  Furthermore, the focus on alignment with human preferences during reinforcement learning and improving the helpfulness and harmlessness aspects of these LLMs has the potential to improve AI interactions in daily life. \n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##Are ethical, safety, or deployment considerations discussed in the paper?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Yes, ethical, safety, and deployment considerations are discussed in the DeepSeek-R1 paper.\n\n**Ethical Considerations:** The paper acknowledges the potential for language mixing issues, particularly when handling queries in languages other than Chinese and English. There is a discussion that DeepSeek-R1 might default to English for reasoning and responses, even if the original query was in another language. The authors also acknowledge potential safety RL related refusals to answer specific queries, demonstrating the importance of responsible AI development.\n\n**Safety Considerations:** The authors emphasize aligning the model with human preferences through a secondary reinforcement learning stage, explicitly aiming to improve the model's helpfulness and harmlessness. They evaluate the model's entire response, including reasoning and summaries, to identify and mitigate potential risks, biases, or harmful content.\n\n**Deployment Considerations:**\nThe paper highlights DeepSeek-R1's sensitivity to prompting styles, noting that few-shot prompting can degrade performance. They recommend that users directly describe the problem and specify the output format using a zero-shot setting for optimal results. This guidance aims to ensure users can effectively leverage the model's capabilities. Additionally, the discussion of distilling the reasoning capability to small dense models and open-sourcing them shows the commitment to the community.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##Is the work reproducible? Are the code, data, and experimental settings sufficiently detailed or openly available?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the paper \"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\" here's an assessment of reproducibility:\n\n**Strengths (Towards Reproducibility):**\n\n*   **Model Release:** The authors open-source DeepSeek-R1-Zero, DeepSeek-R1, and distilled models (1.5B-70B parameters). This is a major boon for reproducibility.\n*   **Algorithm Detail:** GRPO is described with supporting mathematical equations, the reinforcement learning algorithm is adequately detailed.\n*   **Training Data Details:** A description for creating the cold start, SFT, and RL data. The data is fine-tuned starting with the DeepSeek-V3-Base model.\n*   **Benchmark Datasets:** Standard benchmarks used for evaluation are publicly accessible (MMLU, GPQA, etc.). Evaluation prompts can be derived from DeepSeek-V3.\n\n**Weaknesses (Hindering Reproducibility):**\n\n*   **Complete Experimental Settings:** Though some settings are specified, a full comprehensive disclosure of exact hyperparameters and training configurations would be highly valuable.\n*   **Reward model details:** The rule-based reward models and the generative reward model using DeepSeek-V3 are lacking.\n*   **Computational Resources:** The scale of training (model sizes, hardware) is not fully specified, making replication challenging for researchers with limited resources.\n\n**Conclusion:**\n\nThe release of models is a major strength. However, full reproducibility would benefit from a complete specification of the training configuration and specifics of the training data and reward models. Researchers can use the released checkpoints for downstream applications but may struggle to precisely replicate the training process from scratch."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps"
      ],
      "metadata": {
        "id": "4He4uJevbn7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Along the notebook I already mentioned some steps worth doing. Let's gather everything here and add a few more points.\n",
        "\n",
        "* [Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) may help save money and even more time. Right now it only supports Gemini 1.5 family, so waiting for the updates.\n",
        "* [Gemini 2.0 Flash Thinking](https://cloud.google.com/vertex-ai/generative-ai/docs/thinking#thoughts) is a reasoning model similar to OpenAI's o1/o3, Qwen QwQ, or DeepSeek-R1. At the moment, it is experimental and does not support document understanding and PDF inputs. As some of our questions assume reasoning, we may expect even better results with this model.\n",
        "* Processing multiple papers at once. As DeepSeek-R1 paper is built on top of other DeepSeek papers (like DeepSeek-V2 and V3, DeepSeekMath, DeepSeekMoE), it's worth exploring multi-document questions, as more interesting details are in other papers.\n",
        "* More automation along the notebook, e.g. extracting relevant links to other papers, automatically upload papers to a bucket, etc.\n",
        "* Multi-agent pipeline with more specialization along the way. One agent may extract relevant parts of the text, another one may check the code, others may validate the findings, prepare blog posts and publish them.\n"
      ],
      "metadata": {
        "id": "Xy32l4pdbtI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Cloud credits are provided for this project.\n",
        "\n",
        "Author: [Grigory Sapunov](https://www.linkedin.com/in/grigorysapunov/), AI/ML GDE\n",
        "\n",
        "February 23, 2025"
      ],
      "metadata": {
        "id": "V9vZogNkdwUA"
      }
    }
  ]
}